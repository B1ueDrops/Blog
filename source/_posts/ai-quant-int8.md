---
title: AI模型的量化
categories: AI加速
mathjax: true
---

[toc]

## 模型量化的概念

* **模型量化**一般指将**训练完成的模型**的`weight`以及`activation`从`float32`映射到低bit数据类型 (一般是`int8`)的过程.
  * 注意: 模型量化是在推理这一阶段进行的, 训练时必须使用`float32`, 保证梯度准确.
  * `weight`: 一般泛指模型参数, `activation`代表算子的输入输出(计算图的有向边).
* **模型量化的目的:**
  * 优势: 收益较大, 例如大模型.
    * 模型尺寸小.
    * 模型内存占用低.
    * 模型推理速度快.
    * 设备功耗较低.
  * 缺点: 模型精度降低, 不过一般在可以接受范围.



## 模型量化的分类/粒度

### 均匀量化

* 均匀量化: 浮点类型**线性**映射到整型的方式, 也就是说, 对于每一个`int`类型的值, 所有能映射到这个`int`类型的值的`float32`值的个数是固定的.

* 试用场景: 数据的range较大, 数据分布比较均匀, 例如绝大多数CV类模型.

* 分类:

  * 对称量化 (Symmertric quant, Scale quant): 浮点数的0对应到整数的0.
  * 非对称量化 (Asymmetric quant, Affine quant, Zero quant): 浮点数的0不对应到整数的0.

* 均匀量化的公式:

  * `int8`对称量化:
    $$
    S = \frac{abs[max(W_{float})]}{127}\\
    W_{int8} = round(\frac{W_{float}}{S})
    $$

    * 首先计算缩放因子$S$, 缩放因子就是所有`float32`数据的最大值和`int8`最大值的比率.
    * 然后按照这个缩放因子, 将`float32`的数据缩成`int8`.

  * `uint8`对称量化: 和`int8`对称量化一致, 只是将127改成255.

  * `int8`非对称量化: 其中`zp`指的是zero point, 也就是浮点数中的0映射到整数的zp.
    $$
    S = \frac{abs[max(W_{float}) - min(W_{float})]}{127} \\
    zp = -128 - round[\frac{min(W_{float})}{S}]\\
    W_{int8} = -clip[round(\frac{W_{float}}{S}) + zp, -128, 127]
    $$

    * 首先计算缩放因子$S$.
    * 然后, 先将`float32`中的最小值量化到`int8`, 计算量化后的值和`-128`的差距.
    * 后面量化的值, 都需要加上这个差距, 由于加上之后可能超过`int8`的范围, 所以需要强制clip到这个范围.

  * `uint8`非对称量化: 和上面步骤一样, 只是-128换成0, 127换成255.

* 量化粒度: 一般来说, 粒度最细, 准确率越高, 但是计算代价也越高.

  * Per channel: 在图片等数据中, 每一个Channel独立量化, 每个Channel的量化参数不一样, 粒度更细.
  * Per tensor: 每个Tensor都是独立量化.

> 注意: Activation一般不使用Per channel的量化

* 如果使用per channel, 经过神经网络之后的结果无法进行dequantize.

* 均匀量化的精度损失来源:
  * `round`误差: `float`计算后, 要转成整数, 需要`round`操作, 这一步会有精度损失.
  * `clip`误差: 假设一个浮点数很大, 但是`int`类型范围有限, 需要在最大值处截断, 这一步也有精度损失.
  * 如果一个模型在量化之后, 精度损失过大, 那么就是这两种误差出现次数很多.
* Calibration (误差校准):、
  * 观察缩放因子$S$的公式, 如果我们选择了$max(W_{float})$, 但是大部分数据分布都是远离$max(W_{float})$, 那么就会造成很多映射范围没有被占用, 造成精度损失.
  * 因此, 可以不选择$max(W_{float})$, 而选择一个更小的值$T$ (但是会引入更多clip误差, 这也是一个tradeoff).
  * 此时, 对于每一个算子/Activation, 可以拿一小部分数据做推理, 统计所有`float32`数值的出现频率.
  * 然后根据统计结果, 选择分布密集的地方设置$T$, 或者引入信息熵指标(例如KL散度), 解最优化问题, 使得量化后损失最小, 得到最佳的$T$.

> 量化方式和量化粒度的选择标准?

* 对于Weight, 一般用sym quant, 对于Activation, 一般用asym quant.

* 证明: 假设Weight和Activation都使用asym quant, 在进行dequantize的时候:
  $$
  S_{W}(W_{int8} - z_{W}) S_{x}(x_{int8} - z_{x}) = \\
  S_{W}S_{x}W_{int8}x_{int8} - S_{w}z_{W}S_{x}x_{int8} - S_{W}S_{x}z_{x}W_{int8} + S_{W}z_{w}S_{x}z_{x}
  $$

  * 其中, $S_x, z_{w}, S_{x}, z_{x}$都是可以被提前算出来的.
  * $W_{int8}x_{int8}$是sym quant的结果.
  * 但是第二项依赖于$x_{int8}$, 也就是量化后推理的输入, 如果要进行dequantize, 那么我就需要保存这个结果, 并且需要额外的计算.
  * 因此, 可以选择让$z_{W} = 0$, 让第二项消失, dequantize的时候就可以少算一项, 而且可以不用保留任何推理输入.

* 一般来说, 只有含有ReLU的模型有必要考虑`uint8`.
* 需要看CPU/GPU支持哪些数据类型的硬件加速指令, 例如如果只支持`uint8 * int8`的加速, 那么为了匹配指令, activaton就选`uint8`, weight选`int8`.
* 一般来说, 多数模型选per tensor, 精度可以达标.



### 非均匀量化

* 试用场景: 数据分布不均匀, outlier值较多, 例如大语言模型.
  * `LLM.int8()`: 这篇论文证明了大语言模型中, 数据分布不均匀.
