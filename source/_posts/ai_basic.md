---
title: AI的一些基础概念
categories: AI
mathjax: true
---



## Batch

深度学习的训练算法本质上是梯度下降, 更新参数有两个时机:

* 每遍历一个数据, 就计算cost, 然后更新.
* 遍历完整个数据集, 再计算cost, 然后更新.

这两种方法都不是很好, 因此, 采用一种折中的办法, 把数据集分为若干个Batch, 然后遍历完一个Batch之后再更新.



## Batch Normalization

* 神经网络的隐藏层之间, 一般都会加一个非线性的激活函数, 例如sigmoid, tanh, ReLU等等.
  * 这些非线性激活函数有可能只在某个靠近0的小区间内对输入敏感, 但是在其他区间内对输入值不敏感.
  * 因此, 需要将每一层的数据尽可能通过以Batch为单位进行Normalization的方法, 来将输入数据进行归一化后, 再喂给非线性函数.
* 注意: 如果Batch的大小较小, 求出来的均值$\mu$和方差$\delta$可能不具有代表性, 这个时候可以用数据集整体的均值和方差来代替.